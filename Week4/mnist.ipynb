{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the MNIST Classification Problem using the TensorFlow Keras API\n",
    "Learning material: [CS50 AI Lecture 5](https://cs50.harvard.edu/ai/2020/notes/5/)(I had previously taken this course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "# use the MNIST black&white handwriting dataset already present in keras\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() # capital letters to indicate a vectorized version of the whole training data(the X in logistic regression)\n",
    "\n",
    "X_train = X_train/255; X_test = X_test/255 # normalize the pixel values to be in [0, 1]\n",
    "\n",
    "# what does this do?\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "\n",
    "# recast the 60,000x28x28 numpy array x_train into a 60000x28x28x1 numpy array\n",
    "X_train = X_train.reshape(*(X_train.shape), 1)\n",
    "X_test = X_test.reshape(*(X_test.shape), 1)\n",
    "print(\"done loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store Data about our model and how it performed with various values of hyperparameters.\n",
    "# f = open(\"mnist.txt\", mode='a')\n",
    "# f.write('\\n') # write writes to the end of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_26 (Conv2D)          (None, 24, 24, 10)        260       \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 12, 12, 10)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 9, 9, 24)          3864      \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 4, 4, 24)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 384)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 400)               154000    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 400)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                4010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 162,134\n",
      "Trainable params: 162,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1772 - accuracy: 0.9442\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0632 - accuracy: 0.9804\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0460 - accuracy: 0.9859\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0366 - accuracy: 0.9888\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0298 - accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b959c2e0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.layers # for rich support of function definitions in VS Code\n",
    "\n",
    "# Now let us create the NN model\n",
    "model = keras.models.Sequential([\n",
    "    # The convolution layer. 32 of these filters are learnt for a generic image. The input is one of the samples in X_train, which of course has the shape given by X_train.shape[1],[2],[3].\n",
    "    keras.layers.Conv2D(filters=10, kernel_size=(5, 5), activation=\"relu\", input_shape=(28, 28, 1)),#(X_train.shape[1], X_train.shape[2], X_train.shape[3])),\n",
    "    # Pool the convoluted image.\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # another round of convolute and pool.\n",
    "    keras.layers.Conv2D(filters=24, kernel_size=(4, 4), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Flatten the current image. The resulting vector is our input to the neural network. The first layer, if you say.\n",
    "    keras.layers.Flatten(),\n",
    "    # The hidden NN layer\n",
    "    keras.layers.Dense(units=400, activation=\"relu\"),\n",
    "    # Add some dropout to prevent over-reliance on a few nodes.\n",
    "    keras.layers.Dropout(0.4),\n",
    "    # Add an output layer - a neuron for each of the 10 digits.\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "# f.close()\n",
    "# \"compile\" model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "model.fit(x=X_train, y=Y_train, epochs=5)\n",
    "\n",
    "# Note: note that in the model summary we see the Dense Layer having number of nodes(400) * 385 param(eter)s. This is because the input has 384, and so size of vector w is 384, and of course we have the bias parameter b, giving the total of 385 per node. Also note that due to the large number of nodes, the number of parameters is by far the highest in the Dense layer of the whole architecture.\n",
    "# note that the stride is (1, 1) in the Conv Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0246 - accuracy: 0.9920 - 507ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02460998296737671, 0.9919999837875366]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate performance on test data\n",
    "model.evaluate(x=X_test, y=Y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
